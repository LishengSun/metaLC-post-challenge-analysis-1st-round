<h1>Meta-learning from Learning Curves: Evaluation</h1>
<p>In the development phase, submitted agents are meta-trained and meta-tested on the 30&nbsp;datasets of the challenge, using the k-folds meta-cross-validation procedure with k=6 (Figure 1). In each iteration, the meta-data (which includes both validation learning curves and test learning curves, meta-features of datasets, and hyperparameter of algorithms) are given to the agents for meta-learning in any possible way.&nbsp;Then, the trained agents are meta-tested on the remaining 5 datasets.</p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://i.ibb.co/r3LfY2Q/Screenshot-2022-01-03-at-15-09-09.png" alt="" width="618" height="247" /></p>
<p style="text-align: center;" dir="ltr"><strong>Figure 1. How the meta-dataset is used for meta-training and meta-testing the agent</strong></p>
<p dir="ltr"><span>In meta-testing, the agent iteratively interacts with an environment in a Reinforcement Learning style (Figure 2).&nbsp;</span>It suggests actions to reveal the chosen algorithm learning curves progressively.&nbsp;Then, the agent observes the feedback on validation learning curves to decide on its next action. In the development phase, the leaderboard ranking is based on a learning curve established from the result of the most promising algorithm chosen by the agent at each time step, using validation data.&nbsp;The score used for ranking is the area under the learning curve or ALC.</p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://i.ibb.co/rskrJqb/Screenshot-2022-01-03-at-15-57-35.png" alt="" width="820" height="459" /></p>
<p>&nbsp;</p>
<p style="text-align: center;" dir="ltr"><strong>Figure 2. Meta-testing in the DEVELOPMENT PHASE</strong></p>
<p dir="ltr"><span>Everything is similar in the final phase, except that,&nbsp;</span>the learning curve of the agent will be computed using the test learning curves, which are hidden during the development phase to avoid overfitting the test data (Figure 3). The predictions of which algorithm should perform best at each time step will be used to compute the test data learning curves on which they will be evaluated. Having a separate validation set and test set is novel in our protocol and is not common in usual Reinforcement Learning settings.&nbsp;</p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://i.ibb.co/7CrTTfm/Screenshot-2022-01-03-at-15-57-46.png" alt="" width="818" height="435" /></p>
<p style="text-align: center;"><span><span><strong>Figure 3. Meta-testing in the FINAL TEST PHASE</strong></span></span></p>
<h2><strong>Evaluation Metrics</strong></h2>
<p><span id="docs-internal-guid-c13428b0-7fff-4a1e-1a58-ad410998abec"><span>The agent is evaluated by</span></span>&nbsp;the <strong>Area under the agents&rsquo; Learning Curve</strong> (ALC) (Figure 3). The final score used for ranking on the leaderboard is the average test ALC over all meta-test datasets.</p>
